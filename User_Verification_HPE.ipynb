{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "User_Verification_HPE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6Q5tp+UHijPU3T/pG/nON",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HDWilliams/User_Verification_HPE/blob/main/User_Verification_HPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vuXiFy6-iVP",
        "outputId": "6b73c84b-cd82-4963-bcef-805c123a9b0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! git clone https://github.com/haofanwang/accurate-head-pose.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'accurate-head-pose'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 42 (delta 3), reused 2 (delta 1), pack-reused 32\u001b[K\n",
            "Unpacking objects: 100% (42/42), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5pua0d5GXgw"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/utils.py')\n",
        "sys.path.append('/content/hopenet.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSb0EET5DDNQ"
      },
      "source": [
        "#https://github.com/haofanwang/accurate-head-pose/blob/master/test_hopenet.py\n",
        "import sys, os, argparse\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import importlib.util\n",
        "import hopenet\n",
        "import utils\n",
        "from PIL import Image\n",
        "\n",
        "def parse_args():\n",
        "    \"\"\"Parse input arguments.\"\"\"\n",
        "    parser = argparse.ArgumentParser(description='Head pose estimation using the Hopenet network.')\n",
        "    parser.add_argument('--gpu', dest='gpu_id', help='GPU device id to use [0]',\n",
        "            default=0, type=int)\n",
        "    parser.add_argument('--data_dir', dest='data_dir', help='Directory path for data.',\n",
        "          default='/AFLW2000/', type=str)\n",
        "    parser.add_argument('--filename_list', dest='filename_list', help='Path to text file containing relative paths for every example.',\n",
        "          default='/tools/AFLW2000_filename_filtered.txt', type=str)\n",
        "    parser.add_argument('--snapshot', dest='snapshot', help='Name of model snapshot.',\n",
        "          default='/output/snapshots/AFLW2000/_epoch_9.pkl', type=str)\n",
        "    parser.add_argument('--batch_size', dest='batch_size', help='Batch size.',\n",
        "          default=1, type=int)\n",
        "    parser.add_argument('--save_viz', dest='save_viz', help='Save images with pose cube.',\n",
        "          default=False, type=bool)\n",
        "    parser.add_argument('--dataset', dest='dataset', help='Dataset type.', default='AFLW2000', type=str)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def run_pose_detection(model_path, img_PATH):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \"\"\"\n",
        "    cudnn.enabled = True\n",
        "    gpu = args.gpu_id\n",
        "    snapshot_path = args.snapshot\n",
        "    \"\"\"\n",
        "    print(hopenet)\n",
        "    # ResNet50 structure\n",
        "    model = hopenet.Multinet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 198)\n",
        "\n",
        "    # Load snapshot\n",
        "    saved_state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(saved_state_dict)\n",
        "\n",
        "\n",
        "    transformations = transforms.Compose([transforms.Resize(224),\n",
        "    transforms.CenterCrop(224), transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "    #test_loader = torch.utils.data.DataLoader(dataset=pose_dataset, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "    img = Image.open(img_PATH)\n",
        "    img = transformations(img)\n",
        "    img = img.unsqueeze(0)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Test the Model\n",
        "    model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
        "    total = 0\n",
        "\n",
        "    idx_tensor = [idx for idx in range(198)]\n",
        "    idx_tensor = torch.FloatTensor(idx_tensor).to(device)\n",
        "\n",
        "    l1loss = torch.nn.L1Loss(size_average=False)\n",
        "\n",
        "\n",
        "    #for i, (images, labels, labels_0, labels_1, labels_2, labels_3, cont_labels, name) in enumerate(test_loader):\n",
        "    img = Variable(img).to(device)\n",
        "    \n",
        "    yaw,yaw_1,yaw_2,yaw_3,yaw_4, pitch,pitch_1,pitch_2,pitch_3,pitch_4, roll,roll_1,roll_2,roll_3,roll_4 = model(img)\n",
        "\n",
        "    # Binned predictions\n",
        "    _, yaw_bpred = torch.max(yaw.data, 1)\n",
        "    _, pitch_bpred = torch.max(pitch.data, 1)\n",
        "    _, roll_bpred = torch.max(roll.data, 1)\n",
        "\n",
        "    # Continuous predictions\n",
        "    yaw_predicted = utils.softmax_temperature(yaw.data, 1)\n",
        "    pitch_predicted = utils.softmax_temperature(pitch.data, 1)\n",
        "    roll_predicted = utils.softmax_temperature(roll.data, 1)\n",
        "\n",
        "    yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1).cpu() - 99\n",
        "    pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1).cpu() - 99\n",
        "    roll_predicted = torch.sum(roll_predicted * idx_tensor, 1).cpu() - 99\n",
        "    print(f\"Yaw: {yaw_predicted}\")\n",
        "    print(f\"Pitch: {pitch_predicted}\")\n",
        "    print(f\"Roll: {roll_predicted}\")\n",
        "\n",
        "    # Save first image in batch with pose cube or axis.\n",
        "    cv2_img = cv2.imread(img_PATH)\n",
        "        \n",
        "    utils.plot_pose_cube(cv2_img, yaw_predicted[0], pitch_predicted[0], roll_predicted[0], size=100)\n",
        "    utils.draw_axis(cv2_img, yaw_predicted[0], pitch_predicted[0], roll_predicted[0], tdx = 100, tdy= 100, size=100)\n",
        "    cv2.imwrite(('/content/test.jpg'), cv2_img)\n",
        "\n",
        "    return yaw_predicted, pitch_predicted, roll_predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzeM2vL3CHFF"
      },
      "source": [
        "#testing test model function, \n",
        "#writes an image with yaw, pitch and roll estimation vectors and returns vectors for yaw, pitch and roll\n",
        "run_pose_detection('/content/assets/AFLW2000.pkl', '/content/test_img/upper_right.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruGjYjPZpUKL",
        "outputId": "837fe0c3-fce2-40a6-ffa2-f157edba0ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#import xml files for opencv face detection\n",
        "! wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_profileface.xml"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-31 19:19:02--  https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_profileface.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 828514 (809K) [text/plain]\n",
            "Saving to: ‘haarcascade_profileface.xml.1’\n",
            "\n",
            "\r          haarcasca   0%[                    ]       0  --.-KB/s               \rhaarcascade_profile 100%[===================>] 809.10K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-10-31 19:19:02 (6.79 MB/s) - ‘haarcascade_profileface.xml.1’ saved [828514/828514]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isqLGIpnqT_Q"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "def detect_face(img_PATH):\n",
        "  # Load the cascade\n",
        "  face_cascade = cv2.CascadeClassifier('/content/assets/haarcascade_frontalface_default.xml')\n",
        "  # Read the input image\n",
        "  img = cv2.imread(img_PATH)\n",
        "  # Convert into grayscale\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  # Detect faces\n",
        "  faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "\n",
        "  if len(faces) > 1:\n",
        "    print('Multiple faces detected')\n",
        "    return False\n",
        "  elif len(faces) < 1:\n",
        "    print('No faces detected')\n",
        "    return False\n",
        "  # Draw rectangle around the faces\n",
        "  for (x, y, w, h) in faces:\n",
        "      cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "  # Display the output\n",
        "  #cv2_imshow(img)\n",
        "  cv2.waitKey()\n",
        "  return True # TO DO may want to return face at some point as well"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2KNnFJDuN2c"
      },
      "source": [
        "#run get_face\n",
        "print(detect_face('/content/test_img/front.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwrCnxhrv49m"
      },
      "source": [
        "\n",
        "def get_passcode_simp():\n",
        "  #return random angles for yaw, pitch and roll between -45 and 45, and integer determing which axis the angle denotes\n",
        "  #0 = yaw, 1 = pitch, 2 = roll\n",
        "  interval = np.random.choice(2)\n",
        "  if interval:\n",
        "    pass_code = np.random.uniform(-30, -5)\n",
        "  else:\n",
        "    pass_code = np.random.uniform(5, 30)\n",
        "  return np.int(pass_code), np.random.choice(3)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrXV0H0_6bqA"
      },
      "source": [
        "def verify_liveness_simp(initial_img_PATH, test_img_PATH, pass_code, index, margin_of_error = 1, model_PATH = '/content/assets/AFLW2000.pkl'):\n",
        "  if detect_face(initial_img_PATH):\n",
        "    #run pose detection and extract angles\n",
        "    output = run_pose_detection(model_PATH, test_img_PATH) #TO DO, switch input variables for consistency\n",
        "    test_code = np.int(output[index])\n",
        "    if test_code < pass_code + margin_of_error and test_code > pass_code - margin_of_error:\n",
        "      print(\"User Verfied...\")\n",
        "      return True\n",
        "    else:\n",
        "      print(\"Not Verified...\")\n",
        "      return False\n",
        "  else:\n",
        "    #no face or multiple faces detected\n",
        "    return False"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njrbm07a7ye3",
        "outputId": "8da6ba1d-1795-4296-c38b-dc4664d39fbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pass_code, index = get_passcode_simp()\n",
        "print(pass_code, index)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbiskp7_CapD",
        "outputId": "a3043836-f948-4ca3-94ff-28ed4f874a27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "initial_img_path = '/content/test_img/front.jpg'\n",
        "test_img_PATH = '/content/test_img/front.jpg'\n",
        "verify_liveness_simp(initial_img_path, test_img_PATH, pass_code, index)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<module 'hopenet' from '/content/hopenet.py'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Yaw: tensor([-7.3137])\n",
            "Pitch: tensor([3.1826])\n",
            "Roll: tensor([2.0778])\n",
            "Not Verified...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu35YGRtJa5w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}